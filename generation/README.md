This directory helps you finetune and evaluate GPT2 on the whiskey reviews dataset.
## Finetuning GPT2 for review generation
[finetune_whiskeyreviews.py](finetune_whiskeyreviews.py) takes the dataset named `data.csv` stored in [the ../dataset directory](../dataset) (look there to know how to generate that), which should have the following columns:
* whiskey: the name of the whiskey
* rating: int out of 100
* price: int in dollars
* review: text review

And generates a dataset formatted for GPT2 storing it in [../dataset/finetune](../dataset/finetune). The directory will have a `train.csv`, `val.csv`, and `test.csv` by the end of the script. The ratio of samples for each of them is controlled using the `ftrain, fval, ftest` constants.

Each of the csv's has one column called `text` where each row represents one review in the following format
```
f"<price>{row['price']}<rating>{row['rating']}<whiskey>{row['whiskey']}<review>{row['review']}"
```

These files will be then fed into the huggingface GPT2 trainer.

The script also generates a command for you to run which navigates to the [../transformers/examples/language-modeling](../transformers/examples/language-modeling) directory and starts training there with the trainer arguments shown in the command.

After running the training command, you end up with the following
* `finetuned-model-train` directory which houses the training parameters and end results as well as all the checkpoints kept during training and the final model.
* `finetued-model-logs` contains the logs of the training loop. You can use Tensorboard to visualize it by running `tensorboard --logdir finetued-model-logs --bind_all`.

For reference, I ran the training command as generated by `finetune_whiskeyreviews.py` (shown below) for 5 epochs on two Azure K-80's for 31 minutes yielding a final `perplexity = 15.4905`.
```
python -m torch.distributed.launch --nproc_per_node 2 run_clm.py
    --output_dir ../../../generation/finetuned-model-train/ 
    --model_type gpt2 --model_name_or_path gpt2 
    --validation_file ../../../dataset/finetune/val.csv 
    --do_eval --do_train 
    --train_file ../../../dataset/finetune/train.csv 
    --per_device_eval_batch_size 2 
    --per_device_train_batch_size 2 
    --num_train_epochs 5 
    --evaluation_strategy epoch 
    --logging_steps 4 
    --logging_dir ../../../generation/finetued-model-logs 
    --save_strategy epoch 
    --dataloader_num_workers 4
```

## Generation
After finetuning, you can now generate a few reviews. [`run_generation.py`](run_generation.py) is adopted from huggingface and modified slightly to support generation on multiple prompts.

The prompt should follow the same format used during training:
`
f"<price>{row['price']}<rating>{row['rating']}<whiskey>{row['whiskey']}<review>"
`
but now you stop at the `<review>` token and GPT2 will take it from there. You can use the following command to run generation:
```
python run_generation.py 
    --model_type gpt2 
    --model_name_or_path finetuned-model-train/ 
    --prompts ../dataset/finetune/test.csv 
    --quite 
    --output_file generations_test.csv 
    --length 200 
        2> /dev/null
```
`--prompts ../dataset/finetune/val.csv` is going to automatically extract the prompts for the test set.
You can provide your own prompt by removing the `--quite` flag and replacing the `--prompts` flag with `--prompt yourformattedprompt`.

## Sample Generations
These samples are from the test set
```
* "<price>495<rating>89<whiskey>Bainbridge Yama American Single Grain Barley Mizunara Japanese Oak Cask, 45%<review>","Distilled in 2003 and bottled in September 2016. Rum mizunara has been a popular whiskey for a very long time, one worth considering. The nose offers light toffee, sweet orange syrup, lime juice, light honey, sandalwood, and light oak, adding some teasing dried spice notes. The palate is spicy, with caramel and milk chocolate, red currant, and dark fruit. It finishes with coffee, orange rind, and cocoa powder, finishing dry and spicy."
* "<price>225<rating>94<whiskey>Amrut Greedy Angels, 50%<review>","After pushing back into the woods this year, an aggressive and well-integrated expression is finally showing its chest. A blend of Irish oak, red berry, and grape skins, there’s no rush. The nose is fruity and sweet, with fragrant nuts and ripe rhubarb, ginger, nougat, tarry rope, and cocoa. It’s a mix of different Irish oak expressions. But it’s got more depth than the others, so give it a whirl. (6,650 bottles)"
* "<price>130<rating>94<whiskey> Lagavulin 12 year old (Diageo Special Releases 2017), 56.5%<review>","Toreador Extra white oak blends in with bottled classic Lagavulin. Powerful tobacco flavors, leather and silky texture: aromas of mothballs, squid skin, oil, drying grass, cinnamon sticks, and dried spice. Firm and well-balanced, with creamy vanilla and underlying fruit and citrus notes. Mouth-coating, with well-rounded flavor: short, gentle, and sweet. Again, great finish. This needs more effort to fully mature. (U.S. only)"
```

